package producer
import common.Config
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.functions._

object MainApp {

  def main(args: Array[String]): Unit = {
    require(
      args.nonEmpty, "Usage: MainApp [players|scoreboard]"
    )
    require(
      Set("players", "scoreboard").contains(args.head.toLowerCase),
      s"Mode invalide: ${args.head}. Usage: MainApp [players|scoreboard]"
    )

    val spark = SparkSession.builder()
      .appName("ProducerMainApp")
      .master("local[*]") 
      .getOrCreate()

    val mode = args.head.toLowerCase

    // ---- CHOIX TABLE ----
    val tableName =
      if (mode == "players")
        "players"
      else
        "scoreboardplayers"

    val batchSize = 1000
    val intervalSeconds = 1
    
    // RÃ©cupÃ©rer le dernier offset Kafka pour reprendre oÃ¹ on s'est arrÃªtÃ©
    val lastOffset = Utils.getLastOffset(tableName, Config.bootstrap)
    println(s"[Checkpoint] ðŸ“ Dernier offset Kafka pour topic $tableName: $lastOffset")
    
    println(s"[SimStream] Envoi de $batchSize lignes toutes les $intervalSeconds secondes en boucle continue")
    if (lastOffset > 0) {
      println(s"[SimStream] âš ï¸  Reprise depuis l'offset $lastOffset (${lastOffset} lignes dÃ©jÃ  envoyÃ©es)")
    }
    // Charger la table UNE SEULE FOIS au dÃ©marrage
    val data: DataFrame = spark.read.format("jdbc")
      .option("url", Config.pgUrl)
      .option("driver", "org.postgresql.Driver")
      .option("dbtable", tableName)
      .option("user", Config.pgUser)
      .option("password", Config.pgPass)
      .load()

    val dfIndexed = data.withColumn("rowId", monotonically_increasing_id()).cache()
    val totalRows = dfIndexed.count()
    
    println(s"[SimStream] $totalRows lignes chargÃ©es depuis $tableName")
    
    if (totalRows == 0) {    val dfIndexed = data.withColumn("rowId", monotonically_increasing_id()).cache()

      println(s"[SimStream] Aucune donnÃ©e dans la table $tableName, arrÃªt...")
      return
    }
    
    // Calculer le point de dÃ©part : reprendre depuis le checkpoint si disponible
    val startCursor = if (lastOffset > 0) {
      val checkpointRow = math.min(lastOffset, totalRows)
      println(s"[Checkpoint] ðŸ”„ Reprise depuis la ligne $checkpointRow (offset Kafka: $lastOffset)")
      checkpointRow
    } else {
      0L
    }
    
    // Envoyer les donnÃ©es par batch (un seul cycle complet)
    var cursor = startCursor
    while (cursor < totalRows) {
      val batch = dfIndexed.filter(
        col("rowId") >= cursor &&
        col("rowId") < cursor + batchSize
      )

      if (!batch.isEmpty) {
        val forKafka = batch.selectExpr(
          "CAST(rowId AS STRING) AS key",
          "to_json(struct(*)) AS value"
        )

        // Envoi dans Kafka
        forKafka.write
          .format("kafka")
          .option("kafka.bootstrap.servers", Config.bootstrap)
          .option("topic", tableName)
          .save()

        println(s"[SimStream] Lignes $cursor Ã  ${cursor + batchSize - 1} envoyÃ©es (total: $totalRows)")
      }

      cursor += batchSize
      Thread.sleep(intervalSeconds * 1000)
    }
    
    println(s"[SimStream] âœ… TerminÃ© : Toutes les $totalRows lignes ont Ã©tÃ© envoyÃ©es au topic Kafka '$tableName'")
  }
}